{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b822997f-94ee-4f4d-b6d9-f0d4246f7a83",
   "metadata": {},
   "source": [
    "# Get questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e49ae5-dc12-4a40-89ec-518cbc4ee56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How can I create a CDNAPSM Study case for the D-OTS?',\n",
       " 'What is SP7?',\n",
       " 'Who won the ellection?',\n",
       " 'what is baps',\n",
       " 'which quality codes can a value have?',\n",
       " 'What are the corresponding numbers for each quality code?',\n",
       " 'What are the numeric representations for input in the value?',\n",
       " 'What information can you provide about the vau utility?',\n",
       " 'What information can you provide about the \"vau value utility\"?',\n",
       " 'How can I change an analog value using the \"vau value utility\"?',\n",
       " 'How can I set a measured value to invalid using the \"vau value utility\"?',\n",
       " 'How can I change the mark for a measured value using the \"vau value utility\"?',\n",
       " 'What is Condense mode for a unit?',\n",
       " \"What is the mode 'MRN' for units in OTS?\",\n",
       " \"What is the mode 'Must Run' for generators in TNA?\",\n",
       " 'in basidi, where can i find minimum and maximum values',\n",
       " 'Which button should I click to access the minimum and maximum values in BASIDI?',\n",
       " 'how to run updawipe ',\n",
       " 'What is jROS?',\n",
       " 'What are the key components of jROS?',\n",
       " 'how can i change a counter value via bau utility',\n",
       " 'Which command do I have to give in the BUA utility to change the value of a counter?',\n",
       " 'How can I set the current accumulator value of the operating hours of a network component to zero?',\n",
       " 'What does \"SDM\" stand for?',\n",
       " 'Is there anything that could be abbreviated as \"SDM\" that you are aware of?',\n",
       " 'What does \"SPM\" stand for?',\n",
       " 'What does \"BASIDI\" stand for?',\n",
       " 'Tell me a joke.',\n",
       " 'What does SPM stand for?',\n",
       " 'What does TNE stand for?',\n",
       " 'What does SCADA stand for?',\n",
       " 'what is tads',\n",
       " 'How to Configure SPUX',\n",
       " 'What do you mean by \"SPUX\"?',\n",
       " 'SPUX',\n",
       " 'What is the SPUX used for in the IMMImpPrep process?',\n",
       " 'How do I exit Spectrum Unix?',\n",
       " 'What does IMMImpPrep do?',\n",
       " 'How can I configure SPUX?',\n",
       " 'What can you tell me about the process simulation utility?',\n",
       " 'What information can you provide about the psi process simulation utility?',\n",
       " 'What can you tell me about IMMImpPrep?',\n",
       " 'what is SP7',\n",
       " 'What are some key features and limitations of SP7?',\n",
       " 'what does the product_handle do?',\n",
       " 'What information do you have about \"IMMImpPrep\"?',\n",
       " 'What does CreateDorObject refer to?',\n",
       " 'How can I disable the archive feature?',\n",
       " 'how to run IMMImpPrep',\n",
       " 'How does the administrator create an ODV certificate?',\n",
       " 'what is no communication error in ODV',\n",
       " 'Why some Linux systems do not generate a core for the same process?',\n",
       " 'What steps can be taken to ensure that a core file is generated on a Linux UI client?']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('questions.txt', 'r')  \n",
    "file_contents = file.read()  \n",
    "file.close()\n",
    "questions = eval(file_contents)\n",
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c350a1fc-372b-4545-b2a5-82de80640a0e",
   "metadata": {},
   "source": [
    "# Connect to Chroma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40bb8551-139d-41f5-92b5-65d36c0d539d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all Collections: \n",
      "--------------------\n",
      "questoins\n",
      "20231010_3b807404-4554-5ea0-ba55-d0e35d46a3de\n",
      "full_data\n",
      "questions\n",
      "confluence_SWRD\n",
      "confluence_USGCSCD\n",
      "main_SP7v2.30Q3\n",
      "confluence_GDRDSP7\n",
      "confluence_SIGCUI\n",
      "Ordering_full_data\n",
      "PI_questions\n",
      "hypo_questions\n",
      "AS_full_data\n",
      "confluence_MTKA\n",
      "confluence_GGU\n",
      "PI_full_data\n",
      "Scripts\n",
      "PL_full_data\n",
      "sharepoint_SP7\n",
      "20231109_7c188e59-e156-5640-a731-b00a1523f3a3\n",
      "confluence_SP7_all.30Q3\n",
      "confluence_GCENGRGSP7\n",
      "test\n",
      "confluence_GCCOMM\n",
      "TR_full_data\n",
      "confluence_SP7MKTAOPS\n",
      "AS_questions\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from tqdm import tqdm\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import pandas as pd\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from tqdm import tqdm\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "_ = load_dotenv(\n",
    "    find_dotenv(raise_error_if_not_found=False)\n",
    ")\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"] = os.getenv(\"api_type\")\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"api_base\")\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.getenv(\"api_version\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "azure_embeddings = OpenAIEmbeddings(\n",
    "    deployment= \"text-embedding-ada-002\",\n",
    "    chunk_size= 1, \n",
    "    openai_api_type='azure'\n",
    ")\n",
    "\n",
    "CHROMADB_IP = \"localhost\"\n",
    "CHROMADB_PORT = \"8000\"\n",
    "url_base = f\"http://{CHROMADB_IP}:{CHROMADB_PORT}\"\n",
    "\n",
    "\n",
    "client = chromadb.HttpClient(\n",
    "    host=CHROMADB_IP,\n",
    "    port=CHROMADB_PORT,\n",
    "    settings=Settings(allow_reset=True, anonymized_telemetry=False),\n",
    ")\n",
    "print(\"List all Collections: \")\n",
    "print(\"--------------------\")\n",
    "for coll in client.list_collections():\n",
    "    print(coll.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71d61b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(deployment_name= \"gpt-35-turbo\")\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78ebb003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "from langchain_core.callbacks.manager import Callbacks\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.callbacks.manager import CallbackManagerForRetrieverRun\n",
    "from langchain.callbacks.manager import AsyncCallbackManagerForRetrieverRun\n",
    "import multiprocessing\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_community.vectorstores.utils import maximal_marginal_relevance\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomChroma(Chroma):\n",
    "    \"\"\"\n",
    "    the crucial part is, I need a function which also returns the embeddings,\n",
    "    such a function is not implemented by default or is private.\n",
    "    \"\"\"\n",
    "    \n",
    "    def set_collection(self,chroma_dictionary):\n",
    "        try:\n",
    "            self._client.delete_collection(name=\"temp\")\n",
    "        except:\n",
    "            pass\n",
    "        self._collection = self._client.create_collection(\"temp\")\n",
    "        self._collection.add(\n",
    "            ids = chroma_dictionary[\"ids\"],\n",
    "            documents = chroma_dictionary[\"documents\"],\n",
    "            metadatas = chroma_dictionary[\"metadatas\"],\n",
    "            embeddings = chroma_dictionary[\"embeddings\"]\n",
    "        )\n",
    "\n",
    "    def to_langchain_document(self, chroma_doc_list):\n",
    "        lang_chain_docs = []\n",
    "        for i in range(len(chroma_doc_list[\"documents\"][0])):\n",
    "            lang_chain_docs.append(\n",
    "                Document(\n",
    "                    page_content = chroma_doc_list[\"documents\"][0][i],\n",
    "                    metadata = chroma_doc_list[\"metadatas\"][0][i]\n",
    "                )\n",
    "            )\n",
    "        return lang_chain_docs\n",
    "    \n",
    "\n",
    "\n",
    "    def search_by_vector(self, query_vector, k_filter=20, k=5, where=None, where_document=None, include=[\"documents\", \"metadatas\", \"embeddings\"]):\n",
    "        #start = time.perf_counter()\n",
    "        results = self._Chroma__query_collection(\n",
    "            query_embeddings=query_vector,\n",
    "            n_results=k_filter,\n",
    "            where=where,\n",
    "            where_document=where_document,\n",
    "            include = include\n",
    "        )\n",
    "        mmr_selected = maximal_marginal_relevance(\n",
    "            np.array(query_vector, dtype=np.float32),\n",
    "            results[\"embeddings\"][0],\n",
    "            k=k,\n",
    "            lambda_mult=0.5,\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"ids\": [np.array(results[\"ids\"][0])[mmr_selected].tolist()],\n",
    "            \"embeddings\": [np.array(results[\"embeddings\"][0])[mmr_selected].tolist()],\n",
    "            \"documents\": [np.array(results[\"documents\"][0])[mmr_selected].tolist()],\n",
    "            \"metadatas\": [np.array(results[\"metadatas\"][0])[mmr_selected].tolist()]\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "class GridChatRetriver(BaseRetriever):\n",
    "\n",
    "    vectorstores: list\n",
    "    embedding : Any\n",
    "    k1 : int\n",
    "    k2 : int\n",
    "    k_filter : int\n",
    "\n",
    "    def background_task(self, vectorstore, query_vector, k_filter, k2):\n",
    "        return vectorstore.search_by_vector(query_vector, k_filter=k_filter, k=k2)\n",
    "\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -> List[Document]:\n",
    " \n",
    "\n",
    "        pool = multiprocessing.Pool()\n",
    "\n",
    "        query_vector = self.embedding.embed_query(query)\n",
    "\n",
    "        results = pool.starmap(self.background_task,  [(vec, query_vector, self.k_filter, self.k2) for vec in self.vectorstores ])\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        metadatas = []\n",
    "        ids = []\n",
    "        text_chunks = []\n",
    "        embeddings = []\n",
    "\n",
    "        for result in results:\n",
    "            text_chunks.extend(result[\"documents\"][0])\n",
    "            metadatas.extend(result[\"metadatas\"][0])\n",
    "            ids.extend(result[\"ids\"][0])\n",
    "            embeddings.extend(result[\"embeddings\"][0])\n",
    "\n",
    "        dictionary = {\n",
    "            \"ids\" : ids,\n",
    "            \"metadatas\" : metadatas,\n",
    "            \"documents\" : text_chunks,\n",
    "            \"embeddings\" : embeddings,\n",
    "        }\n",
    "\n",
    "        temp_vectorstore = CustomChroma(\n",
    "            client = chromadb.Client()\n",
    "        )\n",
    "        temp_vectorstore.set_collection(dictionary)\n",
    "\n",
    "\n",
    "        return temp_vectorstore.to_langchain_document(temp_vectorstore.search_by_vector(query_vector=query_vector,k=self.k1))\n",
    "    \n",
    "    async def _aget_relevant_documents(\n",
    "        self,\n",
    "        query: str,\n",
    "        *,\n",
    "        run_manager: AsyncCallbackManagerForRetrieverRun,\n",
    "        **kwargs: Any,\n",
    "    ) -> List[Document]:\n",
    "        raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad32d4c-7fda-4e3f-bea4-13015c3a3976",
   "metadata": {},
   "source": [
    "## using GridChat retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e32003c0-08de-4a39-8167-945f0ebb1082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "collection_names = [\n",
    "    \"confluence_SWRD\",\n",
    "    \"confluence_USGCSCD\",\n",
    "    \"confluence_GDRDSP7\",\n",
    "    \"confluence_SIGCUI\",\n",
    "    \"confluence_MTKA\",\n",
    "    \"confluence_GGU\",\n",
    "    \"confluence_GCENGRGSP7\",\n",
    "    \"PI_full_data\",\n",
    "    \"AS_full_data\",\n",
    "    \"PL_full_data\",\n",
    "    \"TR_full_data\",\n",
    "    \"confluence_GCCOMM\",\n",
    "    \"confluence_SP7MKTAOPS\"]\n",
    "\n",
    "\n",
    "vectorstores = []\n",
    "for name in collection_names:\n",
    "    vectorstores.append(CustomChroma(\n",
    "        client=client,\n",
    "        collection_name=name)\n",
    "    )\n",
    "\n",
    "\n",
    "gridchat_retriver = GridChatRetriver(\n",
    "    vectorstores=vectorstores,\n",
    "    embedding=azure_embeddings,\n",
    "    k_filter = 20, #how many vectors are selected using maximal marginal relecance\n",
    "    k1 = 5, #how many selected at the end\n",
    "    k2 = 2 #how many for every collection\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca35aeaa",
   "metadata": {},
   "source": [
    "# testing retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e4940d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53it [04:03,  4.59s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def create_context(prompt):\n",
    "    docs = gridchat_retriver.get_relevant_documents(prompt)\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in docs:\n",
    "        context += doc.page_content + \"\\n\\n\\n\"\n",
    "    return context\n",
    "\n",
    "def get_df():\n",
    "    answers = []\n",
    "    contexts = []\n",
    "    for i,question in tqdm(enumerate(questions)):\n",
    "\n",
    "        llm = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "        context = create_context(question)\n",
    "        output = llm.apply(input_list=[{\n",
    "            \"context\" : context,\n",
    "            \"question\" : question\n",
    "        }])\n",
    "        contexts.append(context)\n",
    "        answers.append(output[0][\"text\"])\n",
    "        #print(i+1,\"/\", len(questions))\n",
    "\n",
    "    data = {\n",
    "        \"question\" : questions,\n",
    "        \"contexts\" : contexts,\n",
    "        \"answer\": answers\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "output_df = get_df()\n",
    "output_df.to_excel(\"default.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluating the Ragas metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    context_relevancy,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    ")\n",
    "from ragas.metrics.critique import harmfulness\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset\n",
    "\n",
    "metrics = [ faithfulness, answer_relevancy, context_relevancy, harmfulness]\n",
    "\n",
    "gridchat_data =  Dataset.from_dict(output_df)\n",
    "result = evaluate(\n",
    "    gridchat_data,\n",
    "    metrics=metrics,\n",
    "    column_map = { \"question\": \"question\",\"contexts\": \"contexts\",\"answer\": \"answer\"}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
